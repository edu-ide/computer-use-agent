"""
ì¿ íŒ¡ ìƒí’ˆ ìˆ˜ì§‘ ì›Œí¬í”Œë¡œìš° - LangGraph ê¸°ë°˜

Letta ë©”ëª¨ë¦¬ í†µí•©:
- ì›Œí¬í”Œë¡œìš° ì‹œì‘ ì‹œ Letta ì—ì´ì „íŠ¸ ìƒì„±
- ê° ë…¸ë“œ ì‹¤í–‰ ì‹œ ë©”ëª¨ë¦¬ ë¸”ë¡ ì—…ë°ì´íŠ¸
- VLM ì—ì´ì „íŠ¸ì— ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ ì „ë‹¬
"""

import asyncio
import logging
from datetime import datetime
from typing import Any, Callable, Dict, List, Optional

from .workflow_base import (
    WorkflowBase,
    WorkflowConfig,
    WorkflowNode,
    WorkflowState,
    NodeResult,
    NodeStatus,
    VLMErrorType,
)
from .common_subgraphs import CommonSubgraphs
from ..services.coupang_db_service import get_coupang_db
from ..services.letta_memory_service import (
    get_letta_memory_service,
    WorkflowMemoryConfig,
    LettaMemoryService,
)
from ..services.node_reuse_analyzer import (
    get_node_reuse_analyzer,
    NodeReuseAnalyzer,
    ReuseDecision,
)
from ..services.orchestrator_service import (
    get_orchestrator_service,
    OrchestratorService,
    ExecutionStrategy,
    ExecutionDecision,
    WorkflowReport,
)
# Note: ErrorActionì€ ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (LangGraph ì—ëŸ¬ í•¸ë“¤ë§ìœ¼ë¡œ ëŒ€ì²´)
from ..services.agent_activity_log import (
    log_vlm,
    log_memory,
    ActivityType,
)
from ..services.vlm_agent_runner import VLMStepLog
from ..models.coupang_models import CoupangProduct
from .standard_nodes import (
    create_navigation_node,
    create_search_node,
    create_analysis_node,
)

logger = logging.getLogger(__name__)


class CoupangCollectWorkflow(WorkflowBase):
    """
    ì¿ íŒ¡ ë¹„ë¡œì¼“ë°°ì†¡ ìƒí’ˆ ìˆ˜ì§‘ ì›Œí¬í”Œë¡œìš°

    Flow:
    1. open_coupang - ì¿ íŒ¡ ì—´ê¸°
    2. search_keyword - í‚¤ì›Œë“œ ê²€ìƒ‰
    3. analyze_page - í˜ì´ì§€ ë¶„ì„ ë° ìƒí’ˆ ìˆ˜ì§‘
    4. next_page - ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™ (loop back to analyze_page)
    5. find_related - ì—°ê´€ í‚¤ì›Œë“œ íƒìƒ‰
    6. complete - ì™„ë£Œ
    """

    def __init__(self, agent_runner=None):
        """
        Args:
            agent_runner: VLMAgentRunner ì¸ìŠ¤í„´ìŠ¤
        """
        super().__init__()
        self._agent_runner = agent_runner
        self._db = get_coupang_db()
        self._on_step_callback: Optional[Callable] = None

        # Letta ë©”ëª¨ë¦¬ ì„œë¹„ìŠ¤
        self._letta: LettaMemoryService = get_letta_memory_service()
        self._letta_initialized: bool = False

        # ë…¸ë“œ ì¬ì‚¬ìš© ë¶„ì„ê¸° (ìë™ í•™ìŠµ)
        self._reuse_analyzer: NodeReuseAnalyzer = get_node_reuse_analyzer()
        self._auto_learn_reuse: bool = True  # ì¬ì‚¬ìš© ì„¤ì • ìë™ í•™ìŠµ í™œì„±í™”

        # Orchestrator ì„œë¹„ìŠ¤ (ì‹¤í–‰ ì „ëµ ê²°ì •)
        self._orchestrator: OrchestratorService = get_orchestrator_service()
        self._use_orchestrator: bool = True  # Orchestrator ì‚¬ìš© ì—¬ë¶€ (8081 ì„œë²„ í•„ìš”)

        # VLMAgentRunnerì— Orchestrator ì£¼ì… (Step-level interventionì„ ìœ„í•´)
        if self._agent_runner and hasattr(self._agent_runner, "_orchestrator") and self._agent_runner._orchestrator is None:
            self._agent_runner._orchestrator = self._orchestrator

    def set_step_callback(self, callback: Callable):
        """ìŠ¤í… ì½œë°± ì„¤ì • - ì‹¤ì‹œê°„ UI ì—…ë°ì´íŠ¸ìš©"""
        self._on_step_callback = callback

    async def _init_letta_memory(self, keyword: str):
        """Letta ë©”ëª¨ë¦¬ ì—ì´ì „íŠ¸ ì´ˆê¸°í™”"""
        if self._letta_initialized:
            return

        try:
            # ì¿ íŒ¡ ì›Œí¬í”Œë¡œìš°ìš© ë©”ëª¨ë¦¬ ì„¤ì • ìƒì„±
            config = WorkflowMemoryConfig.for_coupang(keyword=keyword)

            # Letta ì—ì´ì „íŠ¸ ìƒì„±
            await self._letta.create_workflow_agent(config)
            self._letta_initialized = True
            print(f"[CoupangWorkflow] Letta ë©”ëª¨ë¦¬ ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ: {keyword}")

        except Exception as e:
            print(f"[CoupangWorkflow] Letta ë©”ëª¨ë¦¬ ì´ˆê¸°í™” ì‹¤íŒ¨ (í´ë°± ëª¨ë“œ ì‚¬ìš©): {e}")
            self._letta_initialized = True  # í´ë°± ëª¨ë“œì—ì„œë„ ì´ˆê¸°í™”ëœ ê²ƒìœ¼ë¡œ ì²˜ë¦¬

    async def _update_memory_on_node_start(self, node_name: str, state: WorkflowState):
        """ë…¸ë“œ ì‹œì‘ ì‹œ ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸"""
        if not self._letta_initialized:
            return

        workflow_id = self.config.id
        data = state.get("data", {})
        completed_nodes = state.get("completed_nodes", [])

        # ì§„í–‰ë¥  ê³„ì‚°
        total_nodes = len(self.nodes) - 2  # error_handler, complete ì œì™¸
        progress = int((len(completed_nodes) / total_nodes) * 100) if total_nodes > 0 else 0

        # ì›Œí¬í”Œë¡œìš° ìƒíƒœ ì—…ë°ì´íŠ¸
        await self._letta.update_workflow_state(
            workflow_id=workflow_id,
            current_node=node_name,
            completed_nodes=completed_nodes,
            progress=progress,
            additional_info=f"- í‚¤ì›Œë“œ: {data.get('current_keyword', '-')}",
        )

        # íƒœìŠ¤í¬ ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
        node_config = self._get_node_config(node_name)
        if node_config:
            await self._letta.update_task_context(
                workflow_id=workflow_id,
                status="ì‹¤í–‰ ì¤‘",
                current_goal=node_config.description,
                next_action=node_config.display_name,
            )

    async def _update_memory_on_node_complete(
        self,
        node_name: str,
        state: WorkflowState,
        result: NodeResult,
    ):
        """ë…¸ë“œ ì™„ë£Œ ì‹œ ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸"""
        if not self._letta_initialized:
            return

        workflow_id = self.config.id
        data = state.get("data", {})

        # ìˆ˜ì§‘ ë°ì´í„° ì—…ë°ì´íŠ¸ (analyze_page, save_products ë…¸ë“œ)
        if node_name in ["analyze_page", "save_products"]:
            collected_count = data.get("collected_count", 0)
            pages_analyzed = data.get("pages_analyzed", 0)

            await self._letta.update_collected_data(
                workflow_id=workflow_id,
                total_items=collected_count,
                pages_analyzed=pages_analyzed,
            )

        # ì—ëŸ¬ ë°œìƒ ì‹œ ì—ëŸ¬ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
        if not result.success and result.error:
            await self._letta.add_error(
                workflow_id=workflow_id,
                error=f"[{node_name}] {result.error}",
            )

        # ì¬ì‚¬ìš© í•™ìŠµ: ì‹¤í–‰ ê²°ê³¼ ê¸°ë¡
        if self._auto_learn_reuse:
            await self._record_and_learn_reuse(node_name, state, result)

    def _get_recent_execution_history(self, limit: int = 5) -> List[Dict[str, Any]]:
        """ìµœê·¼ ì‹¤í–‰ ì´ë ¥ ë°˜í™˜ (Orchestrator ì»¨í…ìŠ¤íŠ¸ìš©)"""
        history = []

        # node_logsì—ì„œ ìµœê·¼ ì‹¤í–‰ ì •ë³´ ì¶”ì¶œ
        node_logs = self._current_state.get("node_logs", {})
        completed_nodes = self._current_state.get("completed_nodes", [])

        for node_id in completed_nodes[-limit:]:
            logs = node_logs.get(node_id, [])
            success = len(logs) > 0 and not any(
                "error" in str(log).lower() for log in logs
            )
            history.append({
                "node_id": node_id,
                "success": success,
                "steps_count": len(logs),
            })

        return history

    async def _record_and_learn_reuse(
        self,
        node_name: str,
        state: WorkflowState,
        result: NodeResult,
    ):
        """ì‹¤í–‰ ê²°ê³¼ ê¸°ë¡ ë° ì¬ì‚¬ìš© ì„¤ì • í•™ìŠµ"""
        workflow_id = self.config.id
        execution_id = state.get("execution_id", "")
        parameters = state.get("parameters", {})
        node_logs = self._current_state.get("node_logs", {})
        steps_count = len(node_logs.get(node_name, []))

        # ì‹¤í–‰ ê²°ê³¼ ê¸°ë¡
        self._reuse_analyzer.record_execution(
            node_id=node_name,
            workflow_id=workflow_id,
            execution_id=execution_id,
            success=result.success,
            steps_count=steps_count,
            parameters=parameters,
            data_produced=result.data if result.data else {},
            error=result.error,
        )

        # ë¶„ì„ ë° í•™ìŠµ (ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìŒ“ì´ë©´)
        analysis = self._reuse_analyzer.analyze_node(node_name, workflow_id)

        if analysis.decision != ReuseDecision.UNCERTAIN:
            # Letta ë©”ëª¨ë¦¬ì— í•™ìŠµëœ ì„¤ì • ì €ì¥
            settings = {
                "decision": analysis.decision.value,
                "confidence": analysis.confidence,
                "reason": analysis.reason,
                "reusable": analysis.recommended_reusable,
                "reuse_trace": analysis.recommended_reuse_trace,
                "share_memory": analysis.recommended_share_memory,
                "cache_key_params": analysis.recommended_cache_key_params,
            }

            await self._letta.update_node_reuse_settings(
                workflow_id=workflow_id,
                node_id=node_name,
                settings=settings,
            )

            print(f"[CoupangWorkflow] {node_name}: ì¬ì‚¬ìš© ì„¤ì • í•™ìŠµë¨ - {analysis.decision.value} ({analysis.confidence:.1%})")

    async def _get_memory_context(self) -> str:
        """VLM ì—ì´ì „íŠ¸ì— ì „ë‹¬í•  ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ"""
        if not self._letta_initialized:
            return ""

        return await self._letta.get_context_for_agent(self.config.id)

    async def _cleanup_letta_memory(self):
        """ì›Œí¬í”Œë¡œìš° ì¢…ë£Œ ì‹œ Letta ë©”ëª¨ë¦¬ ì •ë¦¬ (ì•ˆì „ ëª¨ë“œ)"""
        if self._letta_initialized:
            try:
                # íƒ€ì„ì•„ì›ƒ 2ì´ˆë¡œ ì œí•œ
                await asyncio.wait_for(self._letta.cleanup_agent(self.config.id), timeout=2.0)
            except Exception as e:
                print(f"[CoupangWorkflow] ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨ (ë¬´ì‹œë¨): {e}")
            finally:
                self._letta_initialized = False

    @property
    def config(self) -> WorkflowConfig:
        return WorkflowConfig(
            id="coupang-collect",
            name="ì¿ íŒ¡ ìƒí’ˆ ìˆ˜ì§‘",
            description="í‚¤ì›Œë“œë¡œ ì¿ íŒ¡ì—ì„œ ë¡œì¼“ë°°ì†¡ì´ ì•„ë‹Œ ìƒí’ˆì„ ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.",
            icon="ShoppingCart",
            color="#e31937",
            category="e-commerce",
            parameters=[
                {
                    "name": "keyword",
                    "type": "string",
                    "label": "ê²€ìƒ‰ í‚¤ì›Œë“œ",
                    "placeholder": "ì˜ˆ: ë¬´ì„ ì´ì–´í°",
                    "required": True,
                },
                {
                    "name": "max_pages",
                    "type": "number",
                    "label": "ìµœëŒ€ í˜ì´ì§€ ìˆ˜",
                    "default": 5,
                    "min": 1,
                    "max": 20,
                },
                {
                    "name": "follow_related",
                    "type": "boolean",
                    "label": "ì—°ê´€ í‚¤ì›Œë“œ íƒìƒ‰",
                    "default": True,
                },
            ],
        )

    @property
    def nodes(self) -> List[WorkflowNode]:
        # ëª¨ë¸ ID ê°€ì ¸ì˜¤ê¸°
        model_id = "local-qwen3-vl"
        if self._agent_runner and hasattr(self._agent_runner, "model_id"):
            model_id = self._agent_runner.model_id

        return [
            create_search_node(
                name="search_keyword",
                keyword_param="keyword",
                display_name="í‚¤ì›Œë“œ ê²€ìƒ‰",
                description="ì¿ íŒ¡ ì ‘ì† ë° í‚¤ì›Œë“œ ê²€ìƒ‰",
                on_success="analyze_page",
                on_failure="error_handler",
                agent_type="VLMAgent",
                model_id=model_id,
                timeout_sec=90,
                avg_duration_sec=35,
            ),
            create_analysis_node(
                name="analyze_page",
                goal="Analyze the current page, extract non-rocket delivery products, and navigate to the next page.",
                extraction_details="""Target products that DO NOT have "Rocket Delivery" (ë¡œì¼“ë°°ì†¡), "Rocket Jikgu" (ë¡œì¼“ì§êµ¬), or "Rocket Wow" (ë¡œì¼“ì™€ìš°) badges.
   - Extract: Name, Price, URL, Seller Name.""",
                pagination_details="""- Check if a "Next Page" button exists and is active.
   - If found, click it to navigate to the next page.
   - If clicking next page fails, try scrolling down first.""",
                output_details="""-"products": [list of extracted products]
     - "next_page_clicked": true/false""",
                display_name="í˜ì´ì§€ ë¶„ì„",
                description="í˜ì´ì§€ ë¶„ì„, ìƒí’ˆ ìˆ˜ì§‘ ë° ë‹¤ìŒ í˜ì´ì§€ ì´ë™",
                on_success="save_products",
                on_failure="error_handler",
                agent_type="CodeAgent",  # Code Agent ì‚¬ìš©
                model_id="Orchestrator-8B",  # Orchestrator-8B ëª¨ë¸
                node_type="extract_data",  # Code Agentê°€ ì²˜ë¦¬
                clickable=True,  # í´ë¦­í•˜ì—¬ ìƒì„¸ ë³´ê¸° ê°€ëŠ¥
                timeout_sec=240,
                avg_duration_sec=120,
            ),
            WorkflowNode(
                name="save_products",
                display_name="ìƒí’ˆ ì €ì¥",
                description="ìˆ˜ì§‘í•œ ìƒí’ˆì„ DBì— ì €ì¥",
                on_success="analyze_page",  # Default loop back
                on_failure="error_handler",
                node_type="process",
                timeout_sec=30,  # 30ì´ˆ
                avg_duration_sec=2,  # í‰ê·  2ì´ˆ (ë¹ ë¥¸ DB ì‘ì—…)
                clickable=True,  # ìƒì„¸ ë³´ê¸° ê°€ëŠ¥
            ),
            create_analysis_node(
                name="find_related",
                goal="Find and click a related search keyword to expand the search.",
                extraction_details="""Locate the "Related Keywords" (ì—°ê´€ ê²€ìƒ‰ì–´) section or suggested keywords.
   - Identify a keyword that hasn't been searched yet.""",
                pagination_details="""- Click on the new keyword.
   - Verify that a new search results page is loaded.
   - If clicking fails, try scrolling to bring the element into view.""",
                output_details="""- Return "no_related_found" if no related keywords are found.""",
                display_name="ì—°ê´€ í‚¤ì›Œë“œ",
                description="ì—°ê´€ í‚¤ì›Œë“œ íƒìƒ‰",
                on_success="search_keyword",  # ì—°ê´€ í‚¤ì›Œë“œë¡œ ë‹¤ì‹œ ê²€ìƒ‰
                on_failure="complete",  # ì—†ìœ¼ë©´ ì™„ë£Œ
                agent_type="VLMAgent",
                model_id=model_id,
                timeout_sec=60,
                avg_duration_sec=30,
            ),
            WorkflowNode(
                name="complete",
                display_name="ì™„ë£Œ",
                description="ìˆ˜ì§‘ ì™„ë£Œ",
                node_type="end",
                timeout_sec=10,
                avg_duration_sec=1,
            ),
            WorkflowNode(
                name="error_handler",
                display_name="ì—ëŸ¬ ì²˜ë¦¬",
                description="ì—ëŸ¬ ì²˜ë¦¬",
                node_type="error",
                timeout_sec=30,
                avg_duration_sec=5,
            ),
        ]

    @property
    def start_node(self) -> str:
        return "search_keyword"

    async def execute_node(self, node_name: str, state: WorkflowState) -> NodeResult:
        """ë…¸ë“œë³„ ì‹¤í–‰ ë¡œì§ (Letta ë©”ëª¨ë¦¬ í†µí•©)"""

        # ì²« ë²ˆì§¸ ë…¸ë“œì—ì„œ Letta ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
        if node_name == self.start_node:
            keyword = state.get("parameters", {}).get("keyword", "")
            await self._init_letta_memory(keyword)

        # ë…¸ë“œ ì‹œì‘ ì‹œ ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸
        await self._update_memory_on_node_start(node_name, state)

        handlers = {
            "search_keyword": self._search_keyword,
            "analyze_page": self._analyze_page,
            "save_products": self._save_products,
            "find_related": self._find_related,
            "complete": self._complete,
            "error_handler": self._error_handler,
        }

        handler = handlers.get(node_name)
        if handler:
            print(f"[Workflow] Executing node: {node_name}")
            result = await handler(state)
            print(f"[Workflow] Node completed: {node_name} (success={result.success}, next={result.next_node})")

            # ë…¸ë“œ ì™„ë£Œ ì‹œ ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸
            await self._update_memory_on_node_complete(node_name, state, result)

            # ì™„ë£Œ ë˜ëŠ” ì—ëŸ¬ ë…¸ë“œì—ì„œ ë©”ëª¨ë¦¬ ì •ë¦¬
            if node_name in ["complete", "error_handler"]:
                await self._cleanup_letta_memory()

            return result

        return NodeResult(success=False, error=f"Unknown node: {node_name}")

    def _add_step_to_node_logs(self, state: WorkflowState, node_name: str, step_log):
        """ìŠ¤í… ë¡œê·¸ë¥¼ ë…¸ë“œ ë¡œê·¸ì— ì¶”ê°€

        Note: LangGraph stateëŠ” ë¶ˆë³€ì´ë¯€ë¡œ ì§ì ‘ ìˆ˜ì • ëŒ€ì‹  _current_stateë¥¼ ì—…ë°ì´íŠ¸
        """
        # í˜„ì¬ ì›Œí¬í”Œë¡œìš° ì¸ìŠ¤í„´ìŠ¤ì˜ _current_stateì—ì„œ node_logs ê°€ì ¸ì˜¤ê¸°
        if not hasattr(self, '_current_state') or self._current_state is None:
            return {}

        node_logs = self._current_state.get("node_logs", {})
        if node_name not in node_logs:
            node_logs[node_name] = []

        # tool_callsë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
        tool_calls_serializable = []
        if step_log.tool_calls:
            for tc in step_log.tool_calls:
                if hasattr(tc, 'model_dump'):
                    # Pydantic ëª¨ë¸ì¸ ê²½ìš°
                    tool_calls_serializable.append(tc.model_dump())
                elif hasattr(tc, '__dict__'):
                    # ì¼ë°˜ ê°ì²´ì¸ ê²½ìš°
                    tool_calls_serializable.append(str(tc))
                else:
                    tool_calls_serializable.append(str(tc))

        step_data = {
            "step_number": step_log.step_number,
            "timestamp": step_log.timestamp,
            "screenshot": step_log.screenshot,
            "screenshot_after": getattr(step_log, "screenshot_after", None),
            "thought": step_log.thought,
            "action": step_log.action,
            "observation": step_log.observation,
            "error": step_log.error,
            "tool_calls": tool_calls_serializable,
            "orchestrator_feedback": getattr(step_log, "orchestrator_feedback", None),
        }

        # ë””ë²„ê¹…: ìŠ¤í… ë°ì´í„° ë¡œê¹…
        print(f"[StepLog] {node_name} step {step_log.step_number}: "
              f"thought={bool(step_log.thought)}, "
              f"action={bool(step_log.action)}, "
              f"observation={bool(step_log.observation)}, "
              f"screenshot={bool(step_log.screenshot)}")
        if step_log.thought:
            print(f"  -> thought: {step_log.thought[:100]}...")

        node_logs[node_name].append(step_data)

        # _current_stateì— ì§ì ‘ ì—…ë°ì´íŠ¸ (WebSocket ì¡°íšŒ ì‹œ ì¦‰ì‹œ ë°˜ì˜)
        self._current_state["node_logs"] = node_logs

        if self._on_step_callback:
            self._on_step_callback(node_name, step_log)

        return node_logs

    def _get_node_config(self, node_name: str) -> Optional[WorkflowNode]:
        """ë…¸ë“œ ì„¤ì • ê°€ì ¸ì˜¤ê¸°"""
        for node in self.nodes:
            if node.name == node_name:
                return node
        return None

    async def _run_vlm_instruction(
        self,
        instruction: str,
        state: WorkflowState,
        node_name: str,
    ) -> tuple[bool, Dict[str, Any], Optional[str], VLMErrorType]:
        """
        VLM ëª…ë ¹ ì‹¤í–‰ (Orchestrator íŒ¨í„´)

        ì‹¤í–‰ íë¦„:
        1. Orchestratorê°€ ì‹¤í–‰ ì „ëµ ê²°ì •
        2. CACHE_HIT â†’ ì¦‰ì‹œ ë°˜í™˜ (VLM í˜¸ì¶œ ì—†ìŒ!)
        3. RULE_BASED â†’ ê·œì¹™ ê¸°ë°˜ ì‹¤í–‰
        4. LOCAL_MODEL/CLOUD â†’ VLM ì‹¤í–‰
        """
        import time
        start_time = time.time()

        node_config = self._get_node_config(node_name)
        params = state.get("parameters", {})
        decision = None  # Orchestrator decision

        # í˜„ì¬ ì‹¤í–‰ ID (Orchestrator ë¡œê·¸ìš©)
        current_execution_id = state.get("execution_id", self.config.id)

        # === 1. Orchestrator ê²°ì • (ìºì‹œ ì²´í¬ í¬í•¨) ===
        if self._use_orchestrator:
            # ë¹„ë™ê¸° Orchestrator-8B ëª¨ë¸ ì‚¬ìš© (ì„œë²„ ì‹¤í–‰ ì¤‘ì¼ ë•Œ)
            # ì„œë²„ ë¯¸ì‹¤í–‰ì‹œ ìë™ìœ¼ë¡œ ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ fallback
            decision = await self._orchestrator.decide_async(
                workflow_id=self.config.id,
                node_id=node_name,
                instruction=instruction,
                params=params,
                node_config=node_config,
                execution_id=current_execution_id,  # ì‹¤í–‰ ID ì „ë‹¬
                execution_history=self._get_recent_execution_history(),
            )
            
            # Calculate decision time
            decision_time = int((time.time() - start_time) * 1000)

            # Orchestrator ê²°ì •ì„ ë¡œê·¸ì— ê¸°ë¡ (Step 0)
            if decision:
                self._add_step_to_node_logs(state, node_name, VLMStepLog(
                    step_number=0,
                    timestamp=datetime.now().isoformat(),
                    thought=f"Orchestrator Decision: {decision.strategy.value}\nReason: {decision.reason}\nTime: {decision_time}ms",
                    action=f"[Strategy] {decision.strategy.value}",
                    observation=f"Model: {decision.model_id}\nReuse: {decision.reuse_trace}\nTime: {decision_time}ms",
                    orchestrator_feedback={
                        "action": "strategy_selected",
                        "reason": f"{decision.reason} ({decision_time}ms)",
                        "learned_pattern": decision.strategy.value
                    }
                ))

            # CACHE_HIT: ì¦‰ì‹œ ë°˜í™˜! (VLM í˜¸ì¶œ ì—†ìŒ, íŒë‹¨ë„ ìŠ¤í‚µ)
            if decision.strategy == ExecutionStrategy.CACHE_HIT:
                elapsed_ms = int((time.time() - start_time) * 1000)
                print(f"[Orchestrator] {node_name}: âš¡ CACHE_HIT - {elapsed_ms}ms (VLM ìŠ¤í‚µ)")

                cached = decision.cached_result
                return True, cached.get("data", {}), None, VLMErrorType.NONE

            # RULE_BASED: ê·œì¹™ ê¸°ë°˜ ì‹¤í–‰ (VLM ì—†ì´)
            if decision.strategy == ExecutionStrategy.RULE_BASED:
                elapsed_ms = int((time.time() - start_time) * 1000)
                print(f"[Orchestrator] {node_name}: ğŸ“‹ RULE_BASED - {elapsed_ms}ms")
                # TODO: ê·œì¹™ ê¸°ë°˜ ì‹¤í–‰ êµ¬í˜„
                # í˜„ì¬ëŠ” VLMìœ¼ë¡œ í´ë°±
                pass

            # ëª¨ë¸ ì„ íƒ ë¡œê¹…
            print(f"[Orchestrator] {node_name}: ğŸ¤– {decision.strategy.value} "
                  f"(model={decision.model_id}, reason={decision.reason})")

        # === 2. VLM ì‹¤í–‰ì´ í•„ìš”í•œ ê²½ìš° ===
        if not self._agent_runner:
            return True, {}, None, VLMErrorType.NONE

        def on_step(step_log):
            self._add_step_to_node_logs(state, node_name, step_log)

            # Activity Logì— ìŠ¤í… ì™„ë£Œ ê¸°ë¡
            if step_log.step_number > 0:
                log_vlm(
                    ActivityType.EXECUTION,
                    f"ìŠ¤í… {step_log.step_number} ì‹¤í–‰",
                    details={
                        "action": step_log.action,
                        "thought": step_log.thought[:50] + "..." if step_log.thought else None,
                    },
                    execution_id=current_execution_id,
                    node_id=node_name,
                    duration_ms=getattr(step_log, "duration_ms", None),
                )

            # ë™ê¸° ì½œë°±ì—ì„œ ë¹„ë™ê¸° ì‘ì—… ì•ˆì „í•˜ê²Œ ì‹¤í–‰
            try:
                loop = asyncio.get_running_loop()
                loop.create_task(self._update_observation_on_step(node_name, step_log))
            except RuntimeError:
                # ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì—†ìœ¼ë©´ ë™ê¸°ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê±°ë‚˜ ìŠ¤í‚µ
                pass

        # ì¬ì‚¬ìš© ì„¤ì •: Orchestrator íŒë‹¨ > í•™ìŠµëœ ì„¤ì • > ê¸°ë³¸ê°’
        # Orchestratorê°€ ì´ë¯¸ decisionì— ì¬ì‚¬ìš© ì„¤ì •ì„ í¬í•¨í•¨
        if self._use_orchestrator and decision:
            # Orchestrator-8Bê°€ íŒë‹¨í•œ ì¬ì‚¬ìš© ì„¤ì • ì‚¬ìš©
            reusable = decision.reusable
            reuse_trace = decision.reuse_trace
            share_memory = decision.share_memory
            cache_key_params = decision.cache_key_params
            print(f"[Orchestrator] {node_name}: ì¬ì‚¬ìš©ì„¤ì • - "
                  f"reuse_trace={reuse_trace}, share_memory={share_memory}, "
                  f"cache_key_params={cache_key_params}")
        else:
            # Fallback: í•™ìŠµëœ ì„¤ì • ë˜ëŠ” ê¸°ë³¸ê°’
            learned_settings = self._reuse_analyzer.get_recommended_settings(
                node_name, self.config.id
            )
            use_learned = learned_settings.get("confidence", 0) > 0.7

            if use_learned:
                reusable = learned_settings.get("reusable", False)
                reuse_trace = learned_settings.get("reuse_trace", False)
                share_memory = learned_settings.get("share_memory", False)
                cache_key_params = learned_settings.get("cache_key_params", [])
            else:
                reusable = node_config.reusable if node_config else False
                reuse_trace = node_config.reuse_trace if node_config else False
                share_memory = node_config.share_memory if node_config else False
                cache_key_params = node_config.cache_key_params if node_config else []

        # ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸
        memory_context = ""
        if share_memory:
            memory_context = await self._get_memory_context()

        enhanced_instruction = instruction
        if memory_context:
            enhanced_instruction = f"""## ì´ì „ ì‘ì—… ì»¨í…ìŠ¤íŠ¸ (ë©”ëª¨ë¦¬)
{memory_context}

## í˜„ì¬ íƒœìŠ¤í¬
{instruction}"""

        # í™œë™ ë¡œê·¸: VLM ì‹¤í–‰ ì‹œì‘
        log_vlm(
            ActivityType.EXECUTION,
            f"{node_name} ì‹¤í–‰ ì‹œì‘",
            details={
                "instruction": instruction[:100] + "..." if len(instruction) > 100 else instruction,
                "share_memory": share_memory,
                "reuse_trace": reuse_trace,
            },
            execution_id=current_execution_id,
            node_id=node_name,
        )

        # Orchestrator ê²°ì •ì— ë”°ë¥¸ ë¼ìš°íŒ…
        if decision and decision.agent_type == "CodeAgent":
            logger.info(f"[CoupangWorkflow] Code Agentë¡œ ë¼ìš°íŒ…: {node_name} (Orchestrator ê²°ì •)")
            
            from ..services.code_agent_service import get_code_agent
            
            try:
                code_agent = get_code_agent()
                desktop = self._agent_runner._current_agent.desktop if hasattr(self._agent_runner, '_current_agent') else None
                
                if not desktop:
                    logger.error("[CoupangWorkflow] Desktop ì¸ìŠ¤í„´ìŠ¤ ì—†ìŒ")
                    return NodeResult(
                        success=False,
                        data={},
                        error="Desktop not available for Code Agent",
                    )
                
                # Code Agent ì‹¤í–‰
                code_result = await code_agent.extract_data(
                    desktop=desktop,
                    task_description=enhanced_instruction,
                )
                
                elapsed_ms = int((time.time() - start_time) * 1000)
                
                if code_result.get("success"):
                    logger.info(f"[CoupangWorkflow] Code Agent ì„±ê³µ: {elapsed_ms}ms")
                    return NodeResult(
                        success=True,
                        data=code_result.get("data", {}),
                        error=None,
                    )
                else:
                    logger.error(f"[CoupangWorkflow] Code Agent ì‹¤íŒ¨: {code_result.get('error')}")
                    return NodeResult(
                        success=False,
                        data={},
                        error=code_result.get("error", "Code Agent failed"),
                    )
                
            except Exception as e:
                logger.error(f"[CoupangWorkflow] Code Agent ì‹¤í–‰ ì˜¤ë¥˜: {e}")
                return NodeResult(
                    success=False,
                    data={},
                    error=str(e),
                )

        # VLM ì‹¤í–‰ (Orchestratorê°€ VLMAgentë¡œ ê²°ì •í•œ ê²½ìš°)
        result = await self._agent_runner.run_instruction(
            enhanced_instruction,
            on_step=on_step,
            workflow_id=self.config.id,
            node_id=node_name,
            params=params,
            reuse_trace=reuse_trace,
            reusable=reusable,
            cache_key_params=cache_key_params,
            share_memory=share_memory,
        )

        # ì‹¤í–‰ ê²°ê³¼ ê¸°ë¡ (Orchestrator í†µê³„)
        elapsed_ms = int((time.time() - start_time) * 1000)
        if self._use_orchestrator:
            self._orchestrator.record_execution_result(
                workflow_id=self.config.id,
                node_id=node_name,
                strategy=decision.strategy if self._use_orchestrator else ExecutionStrategy.CLOUD_HEAVY,
                model_id=decision.model_id if self._use_orchestrator else None,
                success=result.success,
                actual_time_ms=elapsed_ms,
                actual_cost=0.0,  # TODO: ì‹¤ì œ ë¹„ìš© ê³„ì‚°
            )

        # Orchestratorë¡œ ê²°ê³¼ ê²€ì¦ (ë´‡ ê°ì§€ / ì‹¤íŒ¨ íŒ¨í„´ ì²´í¬)
        if result.success and self._use_orchestrator:
            # ë§ˆì§€ë§‰ ìŠ¤í…ì—ì„œ thought, observation ì¶”ì¶œ
            last_thought = None
            last_observation = None
            final_answer = result.data.get("final_answer") if result.data else None

            if result.steps and len(result.steps) > 0:
                last_step = result.steps[-1]
                last_thought = last_step.thought
                last_observation = last_step.observation

            is_valid, error_type, error_message = self._orchestrator.validate_execution_result(
                workflow_id=self.config.id,
                node_id=node_name,
                result_data=result.data or {},
                final_answer=final_answer,
                last_observation=last_observation,
                last_thought=last_thought,
            )

            if not is_valid:
                log_vlm(
                    ActivityType.ERROR,
                    f"{node_name} ê²€ì¦ ì‹¤íŒ¨: {error_message}",
                    details={
                        "error_type": error_type,
                        "error_message": error_message,
                    },
                    execution_id=current_execution_id,
                    node_id=node_name,
                    duration_ms=elapsed_ms,
                )
                # ì—ëŸ¬ íƒ€ì… ë§¤í•‘
                vlm_error = VLMErrorType.UNKNOWN
                if error_type == "bot_detected":
                    vlm_error = VLMErrorType.BOT_DETECTED
                elif error_type == "page_load_failed":
                    vlm_error = VLMErrorType.PAGE_FAILED
                return False, result.data, error_message, vlm_error

        # í™œë™ ë¡œê·¸: VLM ì‹¤í–‰ ì™„ë£Œ
        if result.success:
            log_vlm(
                ActivityType.EXECUTION,
                f"{node_name} ì™„ë£Œ",
                details={
                    "success": True,
                    "steps_count": len(result.data.get("steps", [])) if result.data else 0,
                    "reused": result.data.get("reused_from_cache", False) if result.data else False,
                },
                execution_id=current_execution_id,
                node_id=node_name,
                duration_ms=elapsed_ms,
            )
            if result.data.get("reused_from_cache"):
                print(f"[CoupangWorkflow] {node_name}: trace ìºì‹œì—ì„œ ì¬ì‚¬ìš©ë¨")
            return True, result.data, None, VLMErrorType.NONE
        else:
            log_vlm(
                ActivityType.ERROR,
                f"{node_name} ì‹¤íŒ¨: {result.error[:50] if result.error else 'Unknown'}",
                details={
                    "success": False,
                    "error": result.error,
                },
                execution_id=current_execution_id,
                node_id=node_name,
                duration_ms=elapsed_ms,
            )
            # VLMì´ ë³´ê³ í•œ ì—ëŸ¬ íƒ€ì… í™•ì¸
            vlm_error = VLMErrorType.UNKNOWN
            error_type = result.data.get("error_type") if result.data else None
            if error_type:
                # VLMì´ [ERROR:TYPE] í˜•ì‹ìœ¼ë¡œ ë³´ê³ í•œ ì—ëŸ¬
                error_type_upper = error_type.upper()
                if error_type_upper == "BOT_DETECTED":
                    vlm_error = VLMErrorType.BOT_DETECTED
                elif error_type_upper == "PAGE_FAILED":
                    vlm_error = VLMErrorType.PAGE_FAILED
                elif error_type_upper == "ACCESS_DENIED":
                    vlm_error = VLMErrorType.ACCESS_DENIED
            return False, result.data, result.error, vlm_error

    # Note: _run_vlm_with_error_handling ë©”ì„œë“œëŠ” ì œê±°ë¨
    # ì—ëŸ¬ í•¸ë“¤ë§ì€ ì´ì œ LangGraph ì¡°ê±´ë¶€ ì—£ì§€ì—ì„œ ì²˜ë¦¬ë©ë‹ˆë‹¤.
    # - workflow_base.pyì˜ _create_router()ì—ì„œ VLM ì—ëŸ¬ íƒ€ì…ì— ë”°ë¼ ë¼ìš°íŒ…
    # - retry_countëŠ” WorkflowStateì—ì„œ ê´€ë¦¬
    # - ê° ë…¸ë“œëŠ” _run_vlm_instruction()ë§Œ í˜¸ì¶œí•˜ë©´ ë¨

    async def run(self, parameters: Dict[str, Any], thread_id: str = "default") -> WorkflowState:
        """
        ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ (Orchestrator ì¶”ì  ë° ë¦¬í¬íŠ¸ ìƒì„± í¬í•¨)

        Args:
            parameters: ì›Œí¬í”Œë¡œìš° íŒŒë¼ë¯¸í„°
            thread_id: ì‹¤í–‰ ìŠ¤ë ˆë“œ ID

        Returns:
            ìµœì¢… ì›Œí¬í”Œë¡œìš° ìƒíƒœ (ë¦¬í¬íŠ¸ í¬í•¨)
        """
        execution_id = f"{self.config.id}-{thread_id}"

        # Orchestrator ì¶”ì  ì‹œì‘
        if self._use_orchestrator:
            total_nodes = len([n for n in self.nodes
                              if n.node_type == "vlm"])  # VLM ë…¸ë“œë§Œ ì¹´ìš´íŠ¸
            self._orchestrator.start_workflow_tracking(
                workflow_id=self.config.id,
                execution_id=execution_id,
                total_nodes=total_nodes,
            )

        try:
            # ê¸°ë³¸ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
            final_state = await super().run(parameters, thread_id)

            # ë¦¬í¬íŠ¸ ìƒì„±
            if self._use_orchestrator:
                try:
                    report = await self._orchestrator.generate_report(
                        execution_id=execution_id,
                        final_status=final_state.get("status", "unknown"),
                    )
                    final_state["report"] = report.to_dict()
                    print(f"\n{report.summary}")
                    if report.recommendations:
                        print("ê¶Œì¥ì‚¬í•­:")
                        for rec in report.recommendations:
                            print(f"  - {rec}")
                except Exception as e:
                    print(f"[Orchestrator] ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")

            return final_state

        finally:
            # ì¶”ë¡  ë¡œê·¸ ì €ì¥ (ì—ì´ì „íŠ¸ ì •ë¦¬ ì „)
            try:
                from cua2_core.routes.workflow_routes import save_reasoning_logs_before_cleanup
                save_reasoning_logs_before_cleanup(execution_id)
            except Exception as e:
                print(f"[Warning] ì¶”ë¡  ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {e}")

            # ì •ë¦¬
            if self._use_orchestrator:
                self._orchestrator.cleanup_execution(execution_id)

    async def _update_observation_on_step(self, node_name: str, step_log):
        """ìŠ¤í… ì‹¤í–‰ ì‹œ ê´€ì°° ê¸°ë¡ ì—…ë°ì´íŠ¸"""
        if not self._letta_initialized:
            return

        try:
            # thoughtë‚˜ observationì´ ìˆìœ¼ë©´ ê¸°ë¡
            observation = step_log.thought or step_log.observation
            if observation:
                await self._letta.add_observation(
                    workflow_id=self.config.id,
                    current_page=node_name,
                    observation=observation[:200],  # 200ìë¡œ ì œí•œ
                )
        except Exception as e:
            # ê´€ì°° ê¸°ë¡ ì‹¤íŒ¨ëŠ” ë¬´ì‹œ (ë©”ì¸ ë¡œì§ì— ì˜í–¥ ì—†ìŒ)
            pass

    async def _open_coupang(self, state: WorkflowState) -> NodeResult:
        """ì¿ íŒ¡ ì›¹ì‚¬ì´íŠ¸ ì—´ê¸°"""
        try:
            instruction = """
            Open Chrome browser and navigate to https://www.coupang.com

            Steps:
            1. Use open_url("https://www.coupang.com") to navigate to Coupang
            2. Wait for the page to fully load
            3. Confirm you see the Coupang homepage
            """

            success, data, error, vlm_error = await self._run_vlm_instruction(
                instruction, state, "open_coupang"
            )

            if not success:
                return NodeResult(success=False, error=error, vlm_error_type=vlm_error)

            return NodeResult(
                success=True,
                data={"coupang_opened": True}
            )
        except Exception as e:
            return NodeResult(success=False, error=str(e))

    async def _search_keyword(self, state: WorkflowState) -> NodeResult:
        """í‚¤ì›Œë“œ ê²€ìƒ‰"""
        try:
            parameters = state.get("parameters", {})
            data = state.get("data", {})

            # í˜„ì¬ í‚¤ì›Œë“œ (ì—°ê´€ í‚¤ì›Œë“œ ë˜ëŠ” ì›ë˜ í‚¤ì›Œë“œ)
            keyword = data.get("current_keyword") or parameters.get("keyword")

            if not keyword:
                return NodeResult(success=False, error="ê²€ìƒ‰ í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤")

            instruction = f"""
            Search for "{keyword}" on Coupang:

            Steps:
            1. Find the search input box at the top of the page
            2. Click on the search box
            3. Type "{keyword}" using write("{keyword}")
            4. Press Enter to search using press(["enter"])
            5. Wait for search results to load using wait(3)
            6. Confirm search results are displayed
            """

            success, data_result, error, vlm_error = await self._run_vlm_instruction(
                instruction, state, "search_keyword"
            )

            if not success:
                return NodeResult(success=False, error=error, vlm_error_type=vlm_error)

            return NodeResult(
                success=True,
                data={
                    "current_keyword": keyword,
                    "current_page": 1,
                    "searched_keywords": state.get("data", {}).get("searched_keywords", []) + [keyword],
                }
            )
        except Exception as e:
            return NodeResult(success=False, error=str(e))

    async def _analyze_page(self, state: WorkflowState) -> NodeResult:
        """í˜ì´ì§€ ë¶„ì„ ë° ìƒí’ˆ ìˆ˜ì§‘"""
        try:
            data = state.get("data", {})
            keyword = data.get("current_keyword")
            current_page = data.get("current_page", 1)

            instruction = f"""
            Analyze the current Coupang search results page for "{keyword}" (Page {current_page}):

            Look at the product listings and identify products that do NOT have:
            - ë¡œì¼“ë°°ì†¡ badge (blue rocket icon)
            - ë¡œì¼“ì§êµ¬ badge
            - ë¡œì¼“ì™€ìš° badge

            These are "ì¼ë°˜ë°°ì†¡" (regular delivery) products.

            Steps:
            1. Scroll down slowly to see all products using scroll(500, 500, "down", 2)
            2. Look for products without rocket badges
            3. For each non-rocket product, extract:
               - Product name (ìƒí’ˆëª…)
               - Price (ê°€ê²©)
               - Product URL or ID if visible
               - Seller name if visible (íŒë§¤ì)
            4. Continue scrolling to see more products
            5. Return a list of found products in JSON format

            Return the products as a JSON array with keys: name, price, url, seller
            """

            success, data_result, error, vlm_error = await self._run_vlm_instruction(
                instruction, state, "analyze_page"
            )

            if not success:
                return NodeResult(success=False, error=error, vlm_error_type=vlm_error)

            # VLMì´ ìˆ˜ì§‘í•œ ìƒí’ˆ ëª©ë¡ (ì‹¤ì œë¡œëŠ” íŒŒì‹± í•„ìš”)
            collected_products = data_result.get("products", [])

            return NodeResult(
                success=True,
                data={
                    "pending_products": collected_products,  # ì €ì¥ ëŒ€ê¸° ìƒí’ˆ
                    "pages_analyzed": data.get("pages_analyzed", 0) + 1,
                }
            )
        except Exception as e:
            return NodeResult(success=False, error=str(e))

    async def _save_products(self, state: WorkflowState) -> NodeResult:
        """ìƒí’ˆ ì €ì¥ ë° ë‹¤ìŒ ë‹¨ê³„ ê²°ì •"""
        data = state.get("data", {})
        products_data = data.get("products", [])
        
        # DB ì €ì¥
        saved_count = 0
        if products_data:
            try:
                # Convert dicts to CoupangProduct objects
                products = []
                for p in products_data:
                    try:
                        product = CoupangProduct(
                            name=p.get("name", ""),
                            price=p.get("price", ""),
                            url=p.get("url", ""),
                            seller=p.get("seller", ""),
                            keyword=data.get("current_keyword", ""),
                            is_rocket=False
                        )
                        products.append(product)
                    except Exception as e:
                        print(f"[SaveProducts] Product conversion error: {e}")

                if products:
                    saved_count = await self._db.save_products(products)
                    print(f"[SaveProducts] Saved {saved_count} products")
            except Exception as e:
                print(f"[SaveProducts] DB Error: {e}")
                # DB ì—ëŸ¬ë‚˜ë„ ì›Œí¬í”Œë¡œìš°ëŠ” ê³„ì† ì§„í–‰
        
        # ëˆ„ì  ì¹´ìš´íŠ¸ ì—…ë°ì´íŠ¸
        total_collected = data.get("collected_count", 0) + saved_count
        pages_analyzed = data.get("pages_analyzed", 0) + 1
        
        # ë‹¤ìŒ í˜ì´ì§€ í´ë¦­ ì—¬ë¶€ í™•ì¸
        next_page_clicked = data.get("next_page_clicked", False)
        max_pages = state.get("parameters", {}).get("max_pages", 5)
        
        # ë‹¤ìŒ ë‹¨ê³„ ê²°ì •
        next_node = "analyze_page"  # ê¸°ë³¸ê°’: ë‹¤ìŒ í˜ì´ì§€ ë¶„ì„
        
        if not next_page_clicked:
            print("[SaveProducts] No next page clicked -> Find related keywords")
            next_node = "find_related"
        elif pages_analyzed >= max_pages:
            print(f"[SaveProducts] Max pages reached ({pages_analyzed}) -> Find related keywords")
            next_node = "find_related"
            
        return NodeResult(
            success=True, 
            next_node=next_node,
            data={
                "collected_count": total_collected,
                "pages_analyzed": pages_analyzed,
                "last_saved": saved_count
            }
        )

    async def _find_related(self, state: WorkflowState) -> NodeResult:
        """ì—°ê´€ í‚¤ì›Œë“œ íƒìƒ‰"""
        try:
            parameters = state.get("parameters", {})
            data = state.get("data", {})

            # ì—°ê´€ í‚¤ì›Œë“œ íƒìƒ‰ ë¹„í™œì„±í™” ì‹œ
            if not parameters.get("follow_related", True):
                return NodeResult(
                    success=False,  # on_failure -> complete
                    data={"follow_related_disabled": True}
                )

            searched_keywords = data.get("searched_keywords", [])

            instruction = f"""
            Find related search keywords on Coupang:

            Steps:
            1. Look for "ì—°ê´€ ê²€ìƒ‰ì–´" or "ê´€ë ¨ ê²€ìƒ‰ì–´" section on the page
            2. Or look for suggested keywords at the top or bottom of results
            3. Find a new keyword that is NOT in this list: {searched_keywords}
            4. If you find a good related keyword, click on it
            5. If no new related keywords exist, report "no_related_found"
            """

            success, data_result, error, vlm_error = await self._run_vlm_instruction(
                instruction, state, "find_related"
            )

            if not success or data_result.get("no_related_found"):
                return NodeResult(
                    success=False,  # on_failure -> complete
                    data={"no_related_keywords": True},
                    vlm_error_type=vlm_error if not success else VLMErrorType.NONE
                )

            new_keyword = data_result.get("related_keyword")
            if new_keyword and new_keyword not in searched_keywords:
                return NodeResult(
                    success=True,  # on_success -> search_keyword
                    data={
                        "current_keyword": new_keyword,
                        "current_page": 1,
                    }
                )

            return NodeResult(
                success=False,  # on_failure -> complete
                data={"no_related_keywords": True}
            )
        except Exception as e:
            return NodeResult(success=False, error=str(e))

    async def _complete(self, state: WorkflowState) -> NodeResult:
        """ìˆ˜ì§‘ ì™„ë£Œ"""
        data = state.get("data", {})

        return NodeResult(
            success=True,
            data={
                "final_collected_count": data.get("collected_count", 0),
                "keywords_searched": data.get("searched_keywords", []),
                "pages_analyzed": data.get("pages_analyzed", 0),
                "completed": True,
            }
        )

    async def _error_handler(self, state: WorkflowState) -> NodeResult:
        """ì—ëŸ¬ ì²˜ë¦¬ ë° ë³µêµ¬ (CommonSubgraphs í™œìš©)"""
        error = state.get("error", "Unknown error")
        vlm_error = state.get("vlm_error_type", VLMErrorType.UNKNOWN)
        retry_count = state.get("retry_count", 0)
        max_retries = 3

        print(f"[CoupangWorkflow] Error handled: {error}, type={vlm_error}, retry={retry_count}")

        # CommonSubgraphs ìœ í‹¸ë¦¬í‹°ë¡œ ì¬ì‹œë„ ì—¬ë¶€ ê²°ì •
        should_retry = CommonSubgraphs.should_retry(state, max_retries)

        if should_retry:
            # ì¬ì‹œë„ ê°€ëŠ¥í•œ ì—ëŸ¬: ì´ì „ ë…¸ë“œë¡œ ë˜ëŒì•„ê°€ê¸°
            current_node = state.get("current_node", "")
            failed_nodes = state.get("failed_nodes", [])

            # ë§ˆì§€ë§‰ ì‹¤íŒ¨ ë…¸ë“œ ì°¾ê¸°
            retry_node = failed_nodes[-1] if failed_nodes else None

            if retry_node:
                print(f"[CoupangWorkflow] ì¬ì‹œë„ ì¤‘: {retry_node} ({retry_count + 1}/{max_retries})")
                return NodeResult(
                    success=True,
                    data={
                        "error": None,
                        "vlm_error_type": None,
                        "retry_count": retry_count + 1,
                        "_retry_action": True,
                    },
                    next_node=retry_node  # ì´ì „ ë…¸ë“œë¡œ ì¬ì‹œë„
                )

        # ë´‡ ê°ì§€ ë“± ì¹˜ëª…ì  ì—ëŸ¬: ì¦‰ì‹œ ì¢…ë£Œ
        if vlm_error == VLMErrorType.BOT_DETECTED:
            print(f"[CoupangWorkflow] âš ï¸ ë´‡ ê°ì§€ë¨ - ì›Œí¬í”Œë¡œìš° ì¤‘ë‹¨")
            # í™œë™ ë¡œê·¸ì— ë´‡ ê°ì§€ ê¸°ë¡
            log_vlm(
                ActivityType.ERROR,
                "ë´‡ ê°ì§€ë¡œ ì¸í•œ ì›Œí¬í”Œë¡œìš° ì¤‘ë‹¨",
                details={"error": error, "vlm_error_type": str(vlm_error)},
                execution_id=state.get("execution_id", ""),
                node_id="error_handler",
            )
            return NodeResult(
                success=True,
                data={"error_handled": True, "original_error": error, "bot_detected": True},
                next_node="complete"
            )

        # ê¸°ë³¸: ë¶€ë¶„ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬ (Graceful Shutdown)
        return NodeResult(
            success=True,
            data={"error_handled": True, "original_error": error},
            next_node="complete"
        )
